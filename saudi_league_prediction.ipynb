{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saudi Pro League Winner Prediction 2025/26\n",
    "### Powered by FotMob Data & Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Dark theme for YouTube-ready visuals\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (14, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 13,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.dpi': 120,\n",
    "    'axes.facecolor': '#1a1a2e',\n",
    "    'figure.facecolor': '#16213e',\n",
    "    'axes.edgecolor': '#e94560',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.15,\n",
    "    'grid.color': '#e94560'\n",
    "})\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'application/json',\n",
    "    'Referer': 'https://www.fotmob.com/'\n",
    "}\n",
    "\n",
    "LEAGUE_ID = 536  # Saudi Pro League\n",
    "print(\"Setup complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Collection from FotMob API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Fetch League Overview (Standings, Stat Links, Fixtures)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fetch main league data\n",
    "league_url = f\"https://www.fotmob.com/api/leagues?id={LEAGUE_ID}\"\n",
    "print(f\"Fetching league overview from: {league_url}\")\n",
    "resp = requests.get(league_url, headers=HEADERS)\n",
    "print(f\"Status: {resp.status_code}\")\n",
    "league_data = resp.json()\n",
    "\n",
    "# Extract standings\n",
    "standings_raw = league_data['table'][0]['data']['table']['all']\n",
    "standings_df = pd.DataFrame(standings_raw)\n",
    "print(f\"\\nFound {len(standings_df)} teams in standings\")\n",
    "print(standings_df[['name', 'played', 'wins', 'draws', 'losses', 'pts']].to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Scrape ALL Team Stat Categories (27 categories)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get all available stat categories\n",
    "stat_categories = league_data['stats']['teams']\n",
    "print(f\"Available stat categories: {len(stat_categories)}\")\n",
    "for i, cat in enumerate(stat_categories):\n",
    "    print(f\"  {i+1}. {cat['header']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fetch every stat category\n",
    "all_stats = {}\n",
    "season_id = None\n",
    "\n",
    "for i, cat in enumerate(stat_categories):\n",
    "    cat_name = cat['header']\n",
    "    # Get the fetch URL from statLink or fetchAllUrl\n",
    "    stat_link = cat.get('fetchAllUrl', '')\n",
    "    if not stat_link and 'statLink' in cat:\n",
    "        stat_link = cat['statLink']\n",
    "\n",
    "    if not stat_link:\n",
    "        print(f\"  Skipping {cat_name} - no URL found\")\n",
    "        continue\n",
    "\n",
    "    # Build full URL\n",
    "    if stat_link.startswith('/'):\n",
    "        url = f\"https://www.fotmob.com{stat_link}\"\n",
    "    elif not stat_link.startswith('http'):\n",
    "        url = f\"https://www.fotmob.com/{stat_link}\"\n",
    "    else:\n",
    "        url = stat_link\n",
    "\n",
    "    # Extract season ID from URL if we don't have it yet\n",
    "    if season_id is None:\n",
    "        parts = stat_link.split('/')\n",
    "        for j, p in enumerate(parts):\n",
    "            if p == 'season' and j + 1 < len(parts):\n",
    "                season_id = parts[j + 1]\n",
    "                break\n",
    "\n",
    "    print(f\"  [{i+1}/{len(stat_categories)}] Fetching {cat_name}...\")\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            # Extract stat list\n",
    "            if 'TopLists' in data:\n",
    "                for top_list in data['TopLists']:\n",
    "                    stat_key = top_list.get('Title', cat_name)\n",
    "                    stat_list = top_list.get('StatList', [])\n",
    "                    all_stats[stat_key] = stat_list\n",
    "                    print(f\"    -> {stat_key}: {len(stat_list)} teams\")\n",
    "            else:\n",
    "                print(f\"    -> Unexpected format\")\n",
    "        else:\n",
    "            print(f\"    -> Status {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Error: {e}\")\n",
    "\n",
    "    time.sleep(1.5)  # Be polite\n",
    "\n",
    "print(f\"\\nTotal stat categories fetched: {len(all_stats)}\")\n",
    "print(f\"Season ID: {season_id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Fetch Match Fixtures & Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract fixtures\n",
    "fixtures_raw = league_data.get('matches', {}).get('allMatches', [])\n",
    "if not fixtures_raw:\n",
    "    fixtures_raw = league_data.get('fixtures', {}).get('allMatches', [])\n",
    "if not fixtures_raw:\n",
    "    # Try to find fixtures in the data\n",
    "    for key in league_data:\n",
    "        if 'match' in key.lower() or 'fixture' in key.lower():\n",
    "            print(f\"Found fixtures key: {key}\")\n",
    "\n",
    "# Parse fixtures\n",
    "fixtures = []\n",
    "for match in fixtures_raw:\n",
    "    try:\n",
    "        home = match.get('home', {})\n",
    "        away = match.get('away', {})\n",
    "        fixtures.append({\n",
    "            'home_team': home.get('name', ''),\n",
    "            'away_team': away.get('name', ''),\n",
    "            'home_score': home.get('score'),\n",
    "            'away_score': away.get('score'),\n",
    "            'status': match.get('status', {}).get('finished', False),\n",
    "            'round': match.get('round', '')\n",
    "        })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "fixtures_df = pd.DataFrame(fixtures)\n",
    "if len(fixtures_df) > 0:\n",
    "    played_df = fixtures_df[fixtures_df['status'] == True]\n",
    "    upcoming_df = fixtures_df[fixtures_df['status'] == False]\n",
    "    print(f\"Total fixtures: {len(fixtures_df)}\")\n",
    "    print(f\"Played: {len(played_df)}, Upcoming: {len(upcoming_df)}\")\n",
    "else:\n",
    "    print(\"Fixtures not found in expected location - will derive from standings\")\n",
    "    played_df = pd.DataFrame()\n",
    "    upcoming_df = pd.DataFrame()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Fetch Historical Seasons Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get historical season links\n",
    "season_links = league_data.get('stats', {}).get('seasonStatLinks', [])\n",
    "print(f\"Historical seasons available: {len(season_links)}\")\n",
    "for s in season_links:\n",
    "    print(f\"  - {s.get('Name', 'Unknown')}: TournamentId={s.get('TournamentId', 'N/A')}\")\n",
    "\n",
    "# Fetch last 3-5 historical seasons standings\n",
    "historical_standings = {}\n",
    "current_season_name = season_links[0]['Name'] if season_links else \"2024/2025\"\n",
    "\n",
    "for s in season_links[1:6]:  # Skip current, get up to 5 past seasons\n",
    "    s_name = s.get('Name', '')\n",
    "    t_id = s.get('TournamentId', '')\n",
    "    if not t_id:\n",
    "        continue\n",
    "\n",
    "    url = f\"https://www.fotmob.com/api/leagues?id={LEAGUE_ID}&season={t_id}\"\n",
    "    print(f\"Fetching {s_name} (TournamentId={t_id})...\")\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        if r.status_code == 200:\n",
    "            hist_data = r.json()\n",
    "            hist_table = hist_data.get('table', [{}])[0].get('data', {}).get('table', {}).get('all', [])\n",
    "            if hist_table:\n",
    "                historical_standings[s_name] = pd.DataFrame(hist_table)\n",
    "                print(f\"  -> {len(hist_table)} teams\")\n",
    "            else:\n",
    "                print(f\"  -> No table data found\")\n",
    "        else:\n",
    "            print(f\"  -> Status {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error: {e}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\nHistorical seasons loaded: {len(historical_standings)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Build Master DataFrame from Standings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start with standings as base\n",
    "master_df = standings_df[['name', 'played', 'wins', 'draws', 'losses', 'pts']].copy()\n",
    "master_df.columns = ['team', 'played', 'wins', 'draws', 'losses', 'pts']\n",
    "\n",
    "# Parse goals from scoresStr (format: \"GF-GA\")\n",
    "if 'scoresStr' in standings_df.columns:\n",
    "    master_df['goals_for'] = standings_df['scoresStr'].apply(lambda x: int(x.split('-')[0]) if isinstance(x, str) and '-' in x else 0)\n",
    "    master_df['goals_against'] = standings_df['scoresStr'].apply(lambda x: int(x.split('-')[1]) if isinstance(x, str) and '-' in x else 0)\n",
    "elif 'goalConDiff' in standings_df.columns:\n",
    "    master_df['goal_diff'] = standings_df['goalConDiff'].astype(int)\n",
    "\n",
    "# Goal difference\n",
    "if 'goals_for' in master_df.columns:\n",
    "    master_df['goal_diff'] = master_df['goals_for'] - master_df['goals_against']\n",
    "\n",
    "# Basic per-match rates\n",
    "master_df['pts_per_match'] = master_df['pts'] / master_df['played']\n",
    "master_df['win_rate'] = master_df['wins'] / master_df['played']\n",
    "master_df['draw_rate'] = master_df['draws'] / master_df['played']\n",
    "master_df['loss_rate'] = master_df['losses'] / master_df['played']\n",
    "\n",
    "if 'goals_for' in master_df.columns:\n",
    "    master_df['goals_per_match'] = master_df['goals_for'] / master_df['played']\n",
    "    master_df['goals_conceded_per_match'] = master_df['goals_against'] / master_df['played']\n",
    "\n",
    "print(f\"Master DataFrame: {master_df.shape}\")\n",
    "master_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Merge All Scraped Stats into Master DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Map scraped stats to master_df\n",
    "stat_mapping = {}\n",
    "for stat_key, stat_list in all_stats.items():\n",
    "    # Create a clean column name\n",
    "    col_name = stat_key.lower().replace(' ', '_').replace('/', '_per_').replace('-', '_')\n",
    "    col_name = col_name.replace('(', '').replace(')', '').replace('%', 'pct')\n",
    "\n",
    "    # Build mapping from team name to stat value\n",
    "    team_stats = {}\n",
    "    for entry in stat_list:\n",
    "        team_name = entry.get('ParticipantName', entry.get('TeamName', ''))\n",
    "        stat_val = entry.get('StatValue', entry.get('SubStatValue', ''))\n",
    "\n",
    "        # Clean stat value - handle percentage strings and comma numbers\n",
    "        if isinstance(stat_val, str):\n",
    "            stat_val = stat_val.replace('%', '').replace(',', '').strip()\n",
    "            try:\n",
    "                stat_val = float(stat_val)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if team_name:\n",
    "            team_stats[team_name] = stat_val\n",
    "\n",
    "    if team_stats:\n",
    "        stat_mapping[col_name] = team_stats\n",
    "\n",
    "print(f\"Stat columns to add: {len(stat_mapping)}\")\n",
    "\n",
    "# Merge stats with fuzzy matching\n",
    "for col_name, team_stats in stat_mapping.items():\n",
    "    values = []\n",
    "    for team in master_df['team']:\n",
    "        # Try exact match first\n",
    "        if team in team_stats:\n",
    "            values.append(team_stats[team])\n",
    "        else:\n",
    "            # Try partial matching\n",
    "            matched = False\n",
    "            for stat_team, val in team_stats.items():\n",
    "                if team.lower() in stat_team.lower() or stat_team.lower() in team.lower():\n",
    "                    values.append(val)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                values.append(np.nan)\n",
    "\n",
    "    master_df[col_name] = values\n",
    "\n",
    "print(f\"\\nMaster DataFrame shape: {master_df.shape}\")\n",
    "print(f\"Columns: {list(master_df.columns)}\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(master_df.isnull().sum()[master_df.isnull().sum() > 0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Engineer Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# xG-based features (if available)\n",
    "xg_cols = [c for c in master_df.columns if 'xg' in c.lower() or 'expected' in c.lower()]\n",
    "print(f\"xG-related columns found: {xg_cols}\")\n",
    "\n",
    "# Try to identify xG and xGA columns\n",
    "xg_col = None\n",
    "xga_col = None\n",
    "for c in xg_cols:\n",
    "    if 'against' in c.lower() or 'conceded' in c.lower() or 'xga' in c.lower():\n",
    "        xga_col = c\n",
    "    elif 'xg' in c.lower():\n",
    "        if xg_col is None:\n",
    "            xg_col = c\n",
    "\n",
    "if xg_col and xga_col:\n",
    "    master_df['xg_diff'] = master_df[xg_col] - master_df[xga_col]\n",
    "    master_df['xg_overperformance'] = master_df.get('goals_for', master_df.get('goals_per_match', 0)) - master_df[xg_col]\n",
    "    print(f\"Created xG features using {xg_col} and {xga_col}\")\n",
    "elif xg_col:\n",
    "    print(f\"Only found xG column: {xg_col}, no xGA column\")\n",
    "\n",
    "# Per-match normalization for counting stats\n",
    "counting_cols = [c for c in master_df.columns if master_df[c].dtype in ['float64', 'int64']\n",
    "                 and c not in ['played', 'wins', 'draws', 'losses', 'pts', 'pts_per_match',\n",
    "                              'win_rate', 'draw_rate', 'loss_rate', 'goals_per_match',\n",
    "                              'goals_conceded_per_match', 'goal_diff']]\n",
    "\n",
    "print(f\"\\nFeature columns: {len(counting_cols)}\")\n",
    "print(f\"Total features in master_df: {master_df.shape[1]}\")\n",
    "master_df.head(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Add Historical Features (Low Weight)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate historical pedigree\n",
    "historical_features = {}\n",
    "\n",
    "for team in master_df['team']:\n",
    "    positions = []\n",
    "    titles = 0\n",
    "    was_champion_last = 0\n",
    "\n",
    "    for i, (season, hist_df) in enumerate(historical_standings.items()):\n",
    "        for idx, row in hist_df.iterrows():\n",
    "            hist_team = row.get('name', '')\n",
    "            if team.lower() in hist_team.lower() or hist_team.lower() in team.lower():\n",
    "                pos = idx + 1  # Position (1-indexed)\n",
    "                positions.append(pos)\n",
    "                if pos == 1:\n",
    "                    titles += 1\n",
    "                if i == 0 and pos == 1:  # Most recent historical season\n",
    "                    was_champion_last = 1\n",
    "                break\n",
    "\n",
    "    avg_position = np.mean(positions) if positions else 10  # Default mid-table\n",
    "    historical_features[team] = {\n",
    "        'hist_avg_position': avg_position,\n",
    "        'hist_titles': titles,\n",
    "        'hist_was_champion_last': was_champion_last,\n",
    "        'hist_seasons_found': len(positions)\n",
    "    }\n",
    "\n",
    "hist_df = pd.DataFrame(historical_features).T\n",
    "hist_df.index.name = 'team'\n",
    "hist_df = hist_df.reset_index()\n",
    "\n",
    "master_df = master_df.merge(hist_df, on='team', how='left')\n",
    "\n",
    "# Fill NaN historical features with defaults\n",
    "master_df['hist_avg_position'] = master_df['hist_avg_position'].fillna(10)\n",
    "master_df['hist_titles'] = master_df['hist_titles'].fillna(0)\n",
    "master_df['hist_was_champion_last'] = master_df['hist_was_champion_last'].fillna(0)\n",
    "master_df['hist_seasons_found'] = master_df['hist_seasons_found'].fillna(0)\n",
    "\n",
    "print(\"Historical features added:\")\n",
    "print(master_df[['team', 'hist_avg_position', 'hist_titles', 'hist_was_champion_last']].to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Calculate Recent Form (from fixtures)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate form from last 5 matches\n",
    "if len(played_df) > 0:\n",
    "    form_data = {}\n",
    "    for team in master_df['team']:\n",
    "        team_matches = played_df[\n",
    "            (played_df['home_team'].str.contains(team, case=False, na=False)) |\n",
    "            (played_df['away_team'].str.contains(team, case=False, na=False))\n",
    "        ].tail(5)\n",
    "\n",
    "        form_points = 0\n",
    "        form_goals = 0\n",
    "        form_conceded = 0\n",
    "        matches_found = 0\n",
    "\n",
    "        for _, match in team_matches.iterrows():\n",
    "            if match['home_score'] is not None and match['away_score'] is not None:\n",
    "                try:\n",
    "                    hs = int(match['home_score'])\n",
    "                    as_ = int(match['away_score'])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                matches_found += 1\n",
    "                is_home = team.lower() in str(match['home_team']).lower()\n",
    "\n",
    "                if is_home:\n",
    "                    form_goals += hs\n",
    "                    form_conceded += as_\n",
    "                    if hs > as_: form_points += 3\n",
    "                    elif hs == as_: form_points += 1\n",
    "                else:\n",
    "                    form_goals += as_\n",
    "                    form_conceded += hs\n",
    "                    if as_ > hs: form_points += 3\n",
    "                    elif hs == as_: form_points += 1\n",
    "\n",
    "        form_data[team] = {\n",
    "            'form_points_last5': form_points,\n",
    "            'form_goals_last5': form_goals,\n",
    "            'form_conceded_last5': form_conceded,\n",
    "            'form_matches': matches_found,\n",
    "            'form_ppg': form_points / max(matches_found, 1)\n",
    "        }\n",
    "\n",
    "    form_df = pd.DataFrame(form_data).T\n",
    "    form_df.index.name = 'team'\n",
    "    form_df = form_df.reset_index()\n",
    "    master_df = master_df.merge(form_df, on='team', how='left')\n",
    "    print(\"Form features added from fixture data\")\n",
    "else:\n",
    "    print(\"No fixture data available - skipping form features\")\n",
    "    master_df['form_ppg'] = master_df['pts_per_match']  # Use overall as proxy\n",
    "\n",
    "print(f\"\\nFinal Master DataFrame: {master_df.shape}\")\n",
    "print(f\"\\nAll columns ({len(master_df.columns)}):\")\n",
    "for i, col in enumerate(master_df.columns):\n",
    "    print(f\"  {i+1}. {col}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Current Standings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Standings bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "sorted_df = master_df.sort_values('pts', ascending=True)\n",
    "\n",
    "colors = ['#e94560' if i >= len(sorted_df) - 3 else '#0f3460' for i in range(len(sorted_df))]\n",
    "bars = ax.barh(sorted_df['team'], sorted_df['pts'], color=colors, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, pts in zip(bars, sorted_df['pts']):\n",
    "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "            f'{int(pts)}', va='center', fontweight='bold', color='white', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Points', fontweight='bold')\n",
    "ax.set_title('Saudi Pro League 2025/26 - Current Standings', fontweight='bold', fontsize=18, color='#e94560')\n",
    "ax.legend(handles=[\n",
    "    mpatches.Patch(color='#e94560', label='Top 3'),\n",
    "    mpatches.Patch(color='#0f3460', label='Others')\n",
    "], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. xG vs Actual Goals"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# xG vs Actual Goals scatter plot\n",
    "if xg_col and 'goals_for' in master_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    x = master_df[xg_col]\n",
    "    y = master_df['goals_for']\n",
    "\n",
    "    ax.scatter(x, y, s=150, c='#e94560', edgecolors='white', linewidth=1.5, zorder=5)\n",
    "\n",
    "    # Add team labels\n",
    "    for _, row in master_df.iterrows():\n",
    "        ax.annotate(row['team'], (row[xg_col], row['goals_for']),\n",
    "                   textcoords=\"offset points\", xytext=(8, 5),\n",
    "                   fontsize=9, color='white', fontweight='bold')\n",
    "\n",
    "    # Diagonal line (xG = Goals)\n",
    "    lims = [min(x.min(), y.min()) - 2, max(x.max(), y.max()) + 2]\n",
    "    ax.plot(lims, lims, '--', color='#e94560', alpha=0.5, label='xG = Actual Goals')\n",
    "\n",
    "    ax.set_xlabel(f'{xg_col} (Expected Goals)', fontweight='bold')\n",
    "    ax.set_ylabel('Actual Goals Scored', fontweight='bold')\n",
    "    ax.set_title('xG vs Actual Goals - Who\\'s Over/Under Performing?', fontweight='bold', fontsize=16, color='#e94560')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"xG data not available for scatter plot\")\n",
    "    if 'goals_for' in master_df.columns and 'goals_against' in master_df.columns:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.scatter(master_df['goals_for'], master_df['goals_against'], s=150, c='#e94560', edgecolors='white')\n",
    "        for _, row in master_df.iterrows():\n",
    "            ax.annotate(row['team'], (row['goals_for'], row['goals_against']),\n",
    "                       textcoords=\"offset points\", xytext=(8, 5), fontsize=9, color='white')\n",
    "        ax.set_xlabel('Goals Scored', fontweight='bold')\n",
    "        ax.set_ylabel('Goals Conceded', fontweight='bold')\n",
    "        ax.set_title('Attack vs Defense', fontweight='bold', fontsize=16, color='#e94560')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Attack vs Defense Quadrant Chart"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if 'goals_per_match' in master_df.columns and 'goals_conceded_per_match' in master_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    x = master_df['goals_per_match']\n",
    "    y = master_df['goals_conceded_per_match']\n",
    "\n",
    "    ax.scatter(x, y, s=180, c=master_df['pts'], cmap='RdYlGn', edgecolors='white', linewidth=1.5, zorder=5)\n",
    "\n",
    "    # Add quadrant lines at mean\n",
    "    ax.axvline(x.mean(), color='#e94560', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y.mean(), color='#e94560', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Labels\n",
    "    for _, row in master_df.iterrows():\n",
    "        ax.annotate(row['team'], (row['goals_per_match'], row['goals_conceded_per_match']),\n",
    "                   textcoords=\"offset points\", xytext=(8, 5),\n",
    "                   fontsize=9, color='white', fontweight='bold')\n",
    "\n",
    "    # Quadrant labels\n",
    "    ax.text(x.max(), y.min(), 'ELITE\\n(Score lots, concede few)', ha='right', va='bottom',\n",
    "            fontsize=10, color='#00ff88', alpha=0.7, fontweight='bold')\n",
    "    ax.text(x.min(), y.max(), 'STRUGGLING\\n(Score few, concede lots)', ha='left', va='top',\n",
    "            fontsize=10, color='#ff4444', alpha=0.7, fontweight='bold')\n",
    "\n",
    "    ax.set_xlabel('Goals Scored per Match', fontweight='bold')\n",
    "    ax.set_ylabel('Goals Conceded per Match', fontweight='bold')\n",
    "    ax.set_title('Attack vs Defense Quadrant', fontweight='bold', fontsize=16, color='#e94560')\n",
    "    ax.invert_yaxis()  # Lower conceded = better = top\n",
    "    plt.colorbar(ax.collections[0], label='Points')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Key Stats Heatmap (Top 8 Teams)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select top 8 teams and key numeric columns for heatmap\n",
    "top8 = master_df.nlargest(8, 'pts')\n",
    "numeric_cols = [c for c in master_df.columns if master_df[c].dtype in ['float64', 'int64']\n",
    "                and c not in ['played', 'hist_seasons_found', 'form_matches']]\n",
    "\n",
    "# Limit to most interesting columns (max 15)\n",
    "if len(numeric_cols) > 15:\n",
    "    # Prioritize: pts, goals, xg, rates, form\n",
    "    priority_keywords = ['pts', 'goal', 'xg', 'win', 'rate', 'form', 'shot', 'pass', 'tackle', 'possess']\n",
    "    scored = []\n",
    "    for col in numeric_cols:\n",
    "        score = sum(1 for kw in priority_keywords if kw in col.lower())\n",
    "        scored.append((col, score))\n",
    "    scored.sort(key=lambda x: -x[1])\n",
    "    numeric_cols = [c for c, s in scored[:15]]\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    heatmap_data = top8.set_index('team')[numeric_cols]\n",
    "\n",
    "    # Normalize each column 0-1\n",
    "    heatmap_norm = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min() + 1e-10)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    sns.heatmap(heatmap_norm, annot=heatmap_data.round(1).values, fmt='', cmap='RdYlGn',\n",
    "                linewidths=1, linecolor='#1a1a2e', ax=ax, cbar_kws={'label': 'Normalized Score'})\n",
    "    ax.set_title('Top 8 Teams - Key Stats Comparison', fontweight='bold', fontsize=16, color='#e94560')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ML Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Prepare Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Target: points per match (we'll project to full season)\n",
    "target = 'pts_per_match'\n",
    "\n",
    "# Select features - exclude identifiers, target, and direct same-season outcome/leakage columns\n",
    "exclude_cols = [\n",
    "    'team', 'pts', 'pts_per_match', 'played', 'wins', 'draws', 'losses',\n",
    "    'win_rate', 'draw_rate', 'loss_rate', 'goal_diff', 'goals_for', 'goals_against',\n",
    "    'goals_per_match', 'goals_conceded_per_match', 'form_points_last5', 'form_matches',\n",
    "    'hist_seasons_found'\n",
    "]\n",
    "feature_cols = [c for c in master_df.columns if c not in exclude_cols\n",
    "                and master_df[c].dtype in ['float64', 'int64']\n",
    "                and master_df[c].notna().sum() > len(master_df) * 0.5]  # At least 50% non-null\n",
    "\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Features ({len(feature_cols)}):\")\n",
    "for f in feature_cols:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = master_df[feature_cols].fillna(0)\n",
    "y = master_df[target]\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Train & Evaluate Models (Leave-One-Out CV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Models to compare\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=5, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, max_depth=3, learning_rate=0.05, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=200, max_depth=3, learning_rate=0.05, random_state=42, verbosity=0),\n",
    "    'Linear Regression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Leave-One-Out CV (perfect for small datasets like 18 teams)\n",
    "loo = LeaveOneOut()\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=loo, scoring='neg_mean_absolute_error')\n",
    "    mae = -scores.mean()\n",
    "    results[name] = {\n",
    "        'MAE': mae,\n",
    "        'Std': scores.std(),\n",
    "        'model': model\n",
    "    }\n",
    "    print(f\"{name}: MAE = {mae:.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "# Best model\n",
    "best_model_name = min(results, key=lambda k: results[k]['MAE'])\n",
    "print(f\"\\nBest model: {best_model_name} (MAE = {results[best_model_name]['MAE']:.4f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Train Best Model on Full Data & Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train best model on full dataset\n",
    "best_model = results[best_model_name]['model']\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    importances = np.abs(best_model.coef_)\n",
    "else:\n",
    "    importances = np.zeros(len(feature_cols))\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, max(8, len(feature_cols) * 0.4)))\n",
    "colors = ['#e94560' if 'hist' in f else '#0f3460' for f in importance_df['feature']]\n",
    "ax.barh(importance_df['feature'], importance_df['importance'], color=colors, edgecolor='white', linewidth=0.5)\n",
    "ax.set_xlabel('Feature Importance', fontweight='bold')\n",
    "ax.set_title(f'Feature Importance ({best_model_name})', fontweight='bold', fontsize=16, color='#e94560')\n",
    "ax.legend(handles=[\n",
    "    mpatches.Patch(color='#e94560', label='Historical features'),\n",
    "    mpatches.Patch(color='#0f3460', label='Current season features')\n",
    "], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check historical feature importance\n",
    "hist_importance = importance_df[importance_df['feature'].str.contains('hist')]['importance'].sum()\n",
    "total_importance = importance_df['importance'].sum()\n",
    "hist_pct = (hist_importance / total_importance * 100) if total_importance > 0 else 0.0\n",
    "print(f\"\\nHistorical features contribute {hist_pct:.1f}% of total importance\")\n",
    "print(\"(Goal: historical should be LOW, current season should dominate)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict points per match for each team\n",
    "master_df['predicted_ppg'] = best_model.predict(X)\n",
    "\n",
    "# Also train all models for comparison\n",
    "for name, result in results.items():\n",
    "    model = result['model']\n",
    "    model.fit(X, y)\n",
    "    master_df[f'pred_{name.lower().replace(\" \", \"_\")}'] = model.predict(X)\n",
    "\n",
    "# Total matches in Saudi Pro League season (18 teams, each plays 34 matches)\n",
    "TOTAL_MATCHES = 34\n",
    "remaining_matches = TOTAL_MATCHES - master_df['played'].values\n",
    "\n",
    "print(\"Predicted Points Per Game vs Actual:\")\n",
    "compare = master_df[['team', 'pts_per_match', 'predicted_ppg', 'played']].copy()\n",
    "compare['projected_pts'] = master_df['pts'] + (master_df['predicted_ppg'] * remaining_matches)\n",
    "compare = compare.sort_values('projected_pts', ascending=False)\n",
    "print(compare.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Monte Carlo Simulation - 10,000 runs\n",
    "np.random.seed(42)\n",
    "N_SIMULATIONS = 10000\n",
    "\n",
    "# Team strength = predicted ppg with some noise\n",
    "team_strengths = master_df.set_index('team')['predicted_ppg']\n",
    "current_points = master_df.set_index('team')['pts']\n",
    "team_remaining = dict(zip(master_df['team'], remaining_matches))\n",
    "\n",
    "championship_wins = {team: 0 for team in master_df['team']}\n",
    "final_points_all = {team: [] for team in master_df['team']}\n",
    "\n",
    "for sim in range(N_SIMULATIONS):\n",
    "    simulated_points = current_points.copy()\n",
    "\n",
    "    for team in master_df['team']:\n",
    "        rem = team_remaining[team]\n",
    "        if rem > 0:\n",
    "            ppg = team_strengths[team]\n",
    "            # Add noise: each match result has variance\n",
    "            # Simulate match-by-match: each match gives 0, 1, or 3 points\n",
    "            noise_ppg = ppg + np.random.normal(0, 0.15)\n",
    "            noise_ppg = max(0, min(3, noise_ppg))\n",
    "\n",
    "            # Simulate remaining matches\n",
    "            match_points = 0\n",
    "            for _ in range(int(rem)):\n",
    "                rand = np.random.random()\n",
    "                # Convert ppg to win/draw/loss probabilities\n",
    "                win_prob = noise_ppg / 3 * 0.85\n",
    "                draw_prob = (1 - win_prob) * 0.4\n",
    "\n",
    "                if rand < win_prob:\n",
    "                    match_points += 3\n",
    "                elif rand < win_prob + draw_prob:\n",
    "                    match_points += 1\n",
    "\n",
    "            simulated_points[team] += match_points\n",
    "\n",
    "    # Record\n",
    "    # Resolve ties fairly instead of always picking the first index.\n",
    "    top_points = simulated_points.max()\n",
    "    tied = simulated_points[simulated_points == top_points].index.tolist()\n",
    "    champion = np.random.choice(tied)\n",
    "    championship_wins[champion] += 1\n",
    "    for team in master_df['team']:\n",
    "        final_points_all[team].append(simulated_points[team])\n",
    "\n",
    "# Calculate probabilities\n",
    "championship_prob = {team: wins/N_SIMULATIONS*100 for team, wins in championship_wins.items()}\n",
    "championship_prob = dict(sorted(championship_prob.items(), key=lambda x: -x[1]))\n",
    "\n",
    "print(f\"Championship Probabilities ({N_SIMULATIONS:,} simulations):\")\n",
    "print(\"-\" * 45)\n",
    "for team, prob in championship_prob.items():\n",
    "    if prob > 0:\n",
    "        avg_pts = np.mean(final_points_all[team])\n",
    "        std_pts = np.std(final_points_all[team])\n",
    "        print(f\"  {team:25s}: {prob:5.1f}%  (Avg pts: {avg_pts:.0f} +/- {std_pts:.1f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Prediction & Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE BIG REVEAL"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Final Predicted Standings\n",
    "master_df['projected_final_pts'] = master_df['pts'] + (master_df['predicted_ppg'] * remaining_matches)\n",
    "master_df['avg_simulated_pts'] = [np.mean(final_points_all[t]) for t in master_df['team']]\n",
    "master_df['std_simulated_pts'] = [np.std(final_points_all[t]) for t in master_df['team']]\n",
    "master_df['championship_prob'] = [championship_prob.get(t, 0) for t in master_df['team']]\n",
    "\n",
    "# Sort by championship probability\n",
    "final_standings = master_df[['team', 'pts', 'played', 'predicted_ppg', 'projected_final_pts',\n",
    "                              'avg_simulated_pts', 'std_simulated_pts', 'championship_prob']].copy()\n",
    "final_standings = final_standings.sort_values('championship_prob', ascending=False)\n",
    "\n",
    "predicted_champion = final_standings.iloc[0]['team']\n",
    "champion_prob = final_standings.iloc[0]['championship_prob']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"  PREDICTED CHAMPION: {predicted_champion}\")\n",
    "print(f\"  Championship Probability: {champion_prob:.1f}%\")\n",
    "print(f\"  Projected Points: {final_standings.iloc[0]['avg_simulated_pts']:.0f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Rank':<5} {'Team':<25} {'Current Pts':<13} {'Proj. Pts':<12} {'Win Prob %':<10}\")\n",
    "print(\"-\" * 65)\n",
    "for i, (_, row) in enumerate(final_standings.iterrows()):\n",
    "    print(f\"{i+1:<5} {row['team']:<25} {int(row['pts']):<13} {row['avg_simulated_pts']:<12.0f} {row['championship_prob']:<10.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Final Visualization Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Championship Probability"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Championship probability bar chart (THE HERO VISUAL)\n",
    "prob_df = final_standings[final_standings['championship_prob'] > 0].sort_values('championship_prob', ascending=True)\n",
    "\n",
    "if len(prob_df) == 0:\n",
    "    prob_df = final_standings.nlargest(5, 'projected_final_pts')\n",
    "    prob_df = prob_df.sort_values('projected_final_pts', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, max(6, len(prob_df) * 0.8)))\n",
    "\n",
    "gradient_colors = plt.cm.RdYlGn(np.linspace(0.2, 0.9, len(prob_df)))\n",
    "bars = ax.barh(prob_df['team'], prob_df['championship_prob'], color=gradient_colors,\n",
    "               edgecolor='white', linewidth=1)\n",
    "\n",
    "for bar, prob in zip(bars, prob_df['championship_prob']):\n",
    "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "            f'{prob:.1f}%', va='center', fontweight='bold', color='white', fontsize=13)\n",
    "\n",
    "ax.set_xlabel('Championship Probability (%)', fontweight='bold', fontsize=14)\n",
    "ax.set_title(f'Who Will Win the Saudi Pro League 2025/26?\\n({N_SIMULATIONS:,} Monte Carlo Simulations)',\n",
    "             fontweight='bold', fontsize=18, color='#e94560')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Predicted Final Points with Error Bars"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predicted points with confidence intervals\n",
    "sorted_final = final_standings.sort_values('avg_simulated_pts', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "colors = ['#e94560' if i >= len(sorted_final) - 3 else '#0f3460' for i in range(len(sorted_final))]\n",
    "\n",
    "ax.barh(sorted_final['team'], sorted_final['avg_simulated_pts'],\n",
    "        xerr=sorted_final['std_simulated_pts'], color=colors,\n",
    "        edgecolor='white', linewidth=0.5, capsize=3, error_kw={'color': 'white', 'linewidth': 1.5})\n",
    "\n",
    "# Mark current points\n",
    "ax.scatter(sorted_final['pts'], sorted_final['team'], color='#00ff88', s=80, zorder=5,\n",
    "           label='Current Points', marker='D')\n",
    "\n",
    "for _, row in sorted_final.iterrows():\n",
    "    ax.text(row['avg_simulated_pts'] + row['std_simulated_pts'] + 1,\n",
    "            row['team'], f\"{row['avg_simulated_pts']:.0f}\",\n",
    "            va='center', fontweight='bold', color='white', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Points', fontweight='bold')\n",
    "ax.set_title('Predicted Final Points (with uncertainty)', fontweight='bold', fontsize=16, color='#e94560')\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Top 3 Contenders Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Radar chart for top 3 contenders\n",
    "top3 = final_standings.nlargest(3, 'championship_prob')\n",
    "radar_cols = ['win_rate', 'goals_per_match', 'pts_per_match']\n",
    "\n",
    "# Add more radar cols if available\n",
    "for col in ['goals_conceded_per_match', 'form_ppg']:\n",
    "    if col in master_df.columns:\n",
    "        radar_cols.append(col)\n",
    "\n",
    "# Add any xG columns\n",
    "if xg_col:\n",
    "    radar_cols.append(xg_col)\n",
    "if xga_col:\n",
    "    radar_cols.append(xga_col)\n",
    "\n",
    "# Ensure we have at least 4 dimensions\n",
    "available_radar = [c for c in radar_cols if c in master_df.columns and master_df[c].notna().all()]\n",
    "if len(available_radar) < 4:\n",
    "    for c in master_df.select_dtypes(include=[np.number]).columns:\n",
    "        if c not in available_radar and c not in ['played', 'pts', 'pts_per_match'] and len(available_radar) < 6:\n",
    "            available_radar.append(c)\n",
    "\n",
    "if len(available_radar) >= 3:\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "    angles = np.linspace(0, 2 * np.pi, len(available_radar), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    colors_radar = ['#e94560', '#00ff88', '#ffd700']\n",
    "\n",
    "    for i, (_, row) in enumerate(top3.iterrows()):\n",
    "        team_name = row['team']\n",
    "        team_data = master_df[master_df['team'] == team_name]\n",
    "\n",
    "        values = []\n",
    "        for col in available_radar:\n",
    "            val = team_data[col].values[0]\n",
    "            # Normalize 0-1 against all teams\n",
    "            col_min = master_df[col].min()\n",
    "            col_max = master_df[col].max()\n",
    "            if col_max > col_min:\n",
    "                # For conceded stats, invert (lower is better)\n",
    "                if 'conceded' in col.lower() or 'against' in col.lower() or 'xga' in col.lower():\n",
    "                    val = 1 - (val - col_min) / (col_max - col_min)\n",
    "                else:\n",
    "                    val = (val - col_min) / (col_max - col_min)\n",
    "            else:\n",
    "                val = 0.5\n",
    "            values.append(val)\n",
    "        values += values[:1]\n",
    "\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=team_name, color=colors_radar[i])\n",
    "        ax.fill(angles, values, alpha=0.15, color=colors_radar[i])\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels([c.replace('_', ' ').title() for c in available_radar], fontsize=9)\n",
    "    ax.set_title('Top 3 Contenders Comparison', fontweight='bold', fontsize=16, color='#e94560', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8d. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "maes = [results[m]['MAE'] for m in model_names]\n",
    "colors = ['#e94560' if m == best_model_name else '#0f3460' for m in model_names]\n",
    "\n",
    "bars = ax.bar(model_names, maes, color=colors, edgecolor='white', linewidth=1)\n",
    "\n",
    "for bar, mae in zip(bars, maes):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "            f'{mae:.4f}', ha='center', fontweight='bold', color='white')\n",
    "\n",
    "ax.set_ylabel('Mean Absolute Error (lower = better)', fontweight='bold')\n",
    "ax.set_title('Model Comparison (Leave-One-Out CV)', fontweight='bold', fontsize=16, color='#e94560')\n",
    "ax.legend(handles=[\n",
    "    mpatches.Patch(color='#e94560', label=f'Best: {best_model_name}'),\n",
    "    mpatches.Patch(color='#0f3460', label='Other models')\n",
    "])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8e. Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"  SAUDI PRO LEAGUE 2025/26 - PREDICTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  Data Source: FotMob API\")\n",
    "print(f\"  Stats Used: {len(feature_cols)} features across {len(all_stats)} stat categories\")\n",
    "print(f\"  Historical Seasons: {len(historical_standings)} seasons analyzed\")\n",
    "print(f\"  Best ML Model: {best_model_name} (MAE: {results[best_model_name]['MAE']:.4f})\")\n",
    "print(f\"  Simulations: {N_SIMULATIONS:,} Monte Carlo runs\")\n",
    "print(f\"\\n  PREDICTED CHAMPION: {predicted_champion}\")\n",
    "print(f\"  Win Probability: {champion_prob:.1f}%\")\n",
    "print(f\"  Projected Points: {final_standings.iloc[0]['avg_simulated_pts']:.0f} +/- {final_standings.iloc[0]['std_simulated_pts']:.1f}\")\n",
    "print(f\"\\n  Top 3 Contenders:\")\n",
    "for i, (_, row) in enumerate(final_standings.head(3).iterrows()):\n",
    "    print(f\"    {i+1}. {row['team']} - {row['championship_prob']:.1f}% chance ({row['avg_simulated_pts']:.0f} pts)\")\n",
    "print(f\"\\n  Historical features weight: {hist_pct:.1f}% (target: LOW)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n  Model by: AI-Powered Saudi League Analysis\")\n",
    "print(\"  Subscribe for more football predictions!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}